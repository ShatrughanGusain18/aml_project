# -*- coding: utf-8 -*-
"""dnn model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_xEj-UL3_fiVQ8AKBYBmsWCtfYP9zpM9
"""

!pip install keras_tuner
import tensorflow as tf
import keras
import numpy as np
import pandas as pd
import re
import keras_tuner
import matplotlib.pyplot as plt
import seaborn as sns
from keras.models import Sequential, Model
from keras.layers import Dense
from tensorflow.keras import layers
from tensorflow.keras.layers import Dropout
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import classification_report,confusion_matrix, f1_score


# Read and divide the data
df = pd.read_csv("/content/drive/MyDrive/research/dataset2.0/file_name.csv")
df=df.drop(’malwares’,axis=1)

df2 = pd.read_csv("/content/drive/MyDrive/research/dataset2.0/system_call.csv")
df = pd.concat([df, df2], axis = 1)
df[’malwares’] = df[’malwares’].apply(lambda x: re.sub(r’\d+’, ’’, x))
ds = df.values
X = ds[:,0:-1]
print(f"Dataset Size: {ds.shape[0]} Rows and {ds.shape[1]}Columns")

df = df.fillna(df.mean())
y=df[’malwares’]

encoder = LabelEncoder()
encoded_Y = encoder.fit_transform(y)

scaler = MinMaxScaler()
scaler.fit(X)
X = scaler.transform(X)

X_train, x_rem, y_train, y_rem = train_test_split(X, encoded_Y,train_size=0.8, random_state=10,stratify=encoded_Y)
x_val, X_test, y_val, y_test = train_test_split(x_rem, y_rem,train_size=0.5, random_state=10,stratify=y_rem)

def call_existing_code(num_layers,units, activation,dropout_rate, lr):
  model = keras.Sequential()
  model.add(layers.Flatten(input_dim = 1675))
  for i in range(1, num_layers):
    model.add(layers.Dense(units = units, activation =activation))
    model.add(keras.layers.Dropout(rate = dropout_rate))
    model.add(Dense(20, activation= "softmax" ))
    model.compile(loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,optimizer = "adam", metrics=[ "accuracy"])
  return model

def build_model(hp):
  num_layers = hp.Int("num_layers", 3, 8)
  units = hp.Int("units", min_value = 32, max_value=1024, step= 32)
  activation = hp.Choice("activation", ["relu"])
  dropout_rate = hp.Float("dropout_rate", 0.2, 0.5, step=0.1)
  lr = hp.Float("lr", min_value=1e-3, max_value=1e-2, sampling="log")
  model = call_existing_code(num_layers = num_layers, units=units, activation=activation, dropout_rate =dropout_rate, lr=lr)
  return model


tuner = keras_tuner.RandomSearch(hypermodel = build_model,
objective = "val_accuracy",
max_trials = 5,
executions_per_trial = 3)
tuner.search(X_train, y_train, epochs=3,validation_data=(x_val,
y_val))
#print the summary of the search space
tuner.search_space_summary()
best_hp = tuner.get_best_hyperparameters()[0]
model = tuner.hypermodel.build(best_hp)

model.fit([X_train], y_train, epochs=150, batch_size = 8,verbose=1, validation_data=([x_val], y_val)

# Accuracy
scores = model.evaluate(X_test, y_test, verbose=0)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

# Processing model for metrics
predictions = model.predict([X_test])
predictions = np.argmax(predictions, axis=-1)
f1 = f1_score(y_test, predictions, average=’weighted’)
print(’F1: ’, "%.2f" % (f1*100))

# convert encoded data to labels
predictions = encoder.inverse_transform(predictions)
y_test_labels = encoder.inverse_transform(y_test)
xlab = [’adload’, ’agent’, ’obfuscator’, ’renos’, ’startpage’, ’vobfus’, ’winwebsec’, ’zeroaccess’, ’benign’]

# Confusion Matrix
cm = confusion_matrix(y_test_labels, predictions) # makes a confusion matrix with no of samples predicted
cm = cm.astype(’float’) / cm.sum(axis=1)[:, np.newaxis] #converts the values topercentage (cell/sum(row))
# figure size, plot heatmap, change the values to percentage
plt.subplots(figsize=(10,10))
# heatmap = sns.heatmap(cm, annot=True, xticklabels = xlab,
yticklabels= xlab, vmin=0, vmax=50, fmt=’.0%’, cmap=’Blues’)
heatmap = sns.heatmap(cm, annot=True, xticklabels = xlab,yticklabels= xlab, cmap=’Blues’)

def get_tpr_fnr_fpr_tnr(cm):
  dict_metric = dict()
  n = len(cm[0])
  row_sums = cm.sum(axis=1)
  col_sums = cm.sum(axis=0)
  array_sum = sum(sum(cm))
  #initialize a blank nested dictionary
  for i in range(1, n+1):
  keys = str(i)
  dict_metric[keys] = {"TPR":0, "FNR":0, "FPR":0, "TNR":0}
  # calculate and store class-wise TPR, FNR, FPR, TNR
  for i in range(n):
  for j in range(n):
  if i == j:
  keys = str(i+1)
  tp = cm[i, j]
  fn = row_sums[i] - cm[i, j]
  dict_metric[keys]["TPR"] = tp / (tp + fn)
  dict_metric[keys]["FNR"] = fn / (tp + fn)
  fp = col_sums[i] - cm[i, j]
  tn = array_sum - tp - fn - fp
  dict_metric[keys]["FPR"] = fp / (fp + tn)
  dict_metric[keys]["TNR"] = tn / (fp + tn)
  return dict_metric


df = pd.DataFrame(get\_tpr\_fnr\_fpr\_tnr(cm)).transpose()
df = df.iloc[:,0:].apply(np.mean)*100
print(df)